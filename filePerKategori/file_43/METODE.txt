Our main result is an unsuper-vised feature selection strategy for which we give worst-casetheoretical guarantees on the generalization power of theresultant classification function fËœ with respect to the classi-fication function f obtained when keeping all the features.
But rather thanemploying the Singular Value Decomposition (which, upontruncation, would result in a small number of dimensions,each of which is a linear combination of up to all of the orig-inal features), we will attempt to choose a small number ofthese features that preserve the relevant geometric structurein the data (or at least in the data insofar as the particularclassification algorithm is concerned).
Our main goal is to choose a small number rof features, where d . r ? n, such that, by using only thoser features, we can obtain good classification quality, both intheory and in practice, when compared to using the full setof n features.
In particular, we would like to solve exactlyor approximately a RLSC problem of the form (4) to get avector to classify successfully a new document according toa classification function of the form (6).
