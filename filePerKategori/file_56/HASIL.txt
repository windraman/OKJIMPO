It keeps the best features, and thus improves the final performance e.g. macro-f1 to 0.92 and simultaneously decreases the computing time for representing the incoming text waiting to be classified dramatically, which is important because it occurs on line and is time-critical.
Feature reduction is really important because without it, the performance value is only 0.73 of macro-f1, much lower than the one gained with reduced feature spaceâ€™ size.
Our method shows a good property. The performance evaluation data are still high when the dimension is reduced to 100, and it gradually inclines to reach a peak with macro-f1 value of 0.9186 when the dimension is 400.
Compared with the classical document frequency incorporated feature filtering methods analyzed in section 4, such as DF, CHI used in our experiment, it shows in Figure 1, that our method can gain higher performance at a very low dimension, and quickly reach a peak, which means much less computing time and almost best performance while the other two classical methods and the like only show a slow ascend tendency from a lower performance evaluation value without a peak until the dimension reaches to almost the original one, which means decreasing the dimension dramatically without great loss of performance is hard to realize.
