Section Title : 
1 Introduction
2 Related Work
3 Bilingual LDA
6 Results and Discussion
7 Conclusions and Future Work
Label: section  StartNode: 1211  EndNode: 1225
1 Introduction
Label: section  StartNode: 3554  EndNode: 3568
2 Related Work
Label: section  StartNode: 6664  EndNode: 6679
3 Bilingual LDA
Label: section  StartNode: 17843  EndNode: 17867
6 Results and Discussion
Label: section  StartNode: 26796  EndNode: 26825
7 Conclusions and Future Work
Label: Abstract  StartNode: 257  EndNode: 334
Abstract. In this paper we study cross-language information retrieval using a
Label: intro  StartNode: 1211  EndNode: 1225
1 Introduction
Label: rel_work  StartNode: 3554  EndNode: 3568
2 Related Work
Label: method  StartNode: 6664  EndNode: 6679
3 Bilingual LDA
Label: exp_result  StartNode: 17843  EndNode: 17867
6 Results and Discussion
Label: conclusion  StartNode: 26796  EndNode: 26825
7 Conclusions and Future Work
Label: References  StartNode: 28331  EndNode: 28342

References
Label: JUDUL  StartNode: 0  EndNode: 92
Cross-Language Information Retrieval with Latent
Topic Models Trained on a Comparable Corpus
Label: NAMA  StartNode: 93  EndNode: 103
Ivan Vulic
Label: NAMA  StartNode: 106  EndNode: 117
Wim De Smet
Label: NAMA  StartNode: 123  EndNode: 143
Marie-Francine Moens
Label: OTHER  StartNode: 144  EndNode: 196
Department of Computer Science, K.U. Leuven, Belgium
Label: PROBLEM  StartNode: 1769  EndNode: 1974
Translation dictionaries do not exist for every language
pair, and they are usually trained on large parallel corpora, where each document has
an exact translation in the other language, or are hand-built.
Label: PROBLEM  StartNode: 1975  EndNode: 2033
Parallel corpora are not
available for each language pair.
Label: PROBLEM  StartNode: 2034  EndNode: 2183
In contrast, comparable corpora in which documents
in the source and the target language contain similar content, are usually available in
abundance.
Label: PROBLEM  StartNode: 2184  EndNode: 2366
In this paper we address the question whether suitable cross-language re-
trieval models can be built based on the interlingual topic representations learned from
comparable corpora.
Label: METODE  StartNode: 2367  EndNode: 2567
We accomplish this goal by means of a cross-language generative
model, i.e., bilingual Latent Dirichlet Allocation (BiLDA), trained on a comparable cor-
pus such as one composed of Wikipedia articles.
Label: METODE  StartNode: 2568  EndNode: 2688
The resulting probabilistic translation
model is incorporated in a statistical language model for information retrieval.
Label: METODE  StartNode: 6680  EndNode: 6801
The topic model we use is a bilingual extension of a standard LDA model, called bilin-
gual LDA (BiLDA) ([17, 14, 7, 2]).
Label: METODE  StartNode: 14019  EndNode: 14063
We name this model the simple unigram model.
Label: METODE  StartNode: 14064  EndNode: 14186
We can now combine this document model with the LDA-only model using linear
interpolation and the Jelinek-Mercer smoothing
Label: METODE  StartNode: 14479  EndNode: 14520
We call this model the LDA-unigram
model.
Label: DATA  StartNode: 15474  EndNode: 15660
The first subset of our training data is the Europarl corpus [11], extracted from
proceedings of the European Parliament and consisting of 6, 206 parallel documents in
English and Dutch.
Label: DATA  StartNode: 15803  EndNode: 15916
Another training subset is collected from Wikipedia dumps3 and consists of paired
documents in English and Dutch.
Label: DATA  StartNode: 16500  EndNode: 16768
Our experiments have been conducted on three data sets taken from the CLEF 2001-
2003 CLIR campaigns: the LA Times 1994 (LAT), the LA Times 1994 and Glasgow
Herald 1995 (LAT+GH) in English, and the NRC Handelsblad 94-95 and the Algemeen
Dagblad 94-95 (NC+AD) in Dutch.
Label: HASIL  StartNode: 25364  EndNode: 25578
As the corresponding figures show, the LDA-only model seems to be too
coarse to be used as the only component of an IR model (e.g., due to its limited number
of topics, words in queries unobserved during training).
Label: HASIL  StartNode: 25579  EndNode: 25902
However, the combination of
the LDA-only and the simple unigram model, which allows retrieving relevant docu-
ments based on shared words across the languages (e.g. personal names), leads to much
better scores which are competitive even with models which utilize cross-lingual dic-
tionaries or machine translation systems.
Label: HASIL  StartNode: 25903  EndNode: 26247
For instance, our LDA-unigram model would
have been placed among the top 5 retrieval systems for the CLEF 2002 Bilingual to
Dutch task, would have been placed among the top 3 retrieval systems for the CLEF
2001 Bilingual to Dutch task, and outperforms the only participating system in the
CLEF 2002 Dutch to English task (MAP: 0.1495) [20, 21].
Label: HASIL  StartNode: 27296  EndNode: 27672
We have thoroughly evaluated this cross-language retrieval model using standard
test collections from the CLEF 2001-2003 CLIR campaigns and have shown that our
combined model, which fuses evidence from the BiLDA model and the unigram model,
is competitive with the current top CLIR systems that use translation resources that are
hand-built or are trained on parallel corpora.
Label: Problem Title  StartNode: 0  EndNode: 36
Cross-Language Information Retrieval
Label: Method Title  StartNode: 42  EndNode: 92
Latent
Topic Models Trained on a Comparable Corpus
